{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "### Autor: Vinícius dos Santos Mello (viniciusdsmello@poli.ufrj.br)\n",
    "### Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name ClassificationAnalysisMultiProcessing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c83aacf91aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mFunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationAnalysisMultiProcessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclass_anal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name ClassificationAnalysisMultiProcessing"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from Functions import ClassificationAnalysisMultiProcessing as class_anal\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'NeuralNetwork'\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "pict_results_path = results_path+'/'+analysis_name+'/picts'\n",
    "files_results_path = results_path+'/'+analysis_name+'/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = True\n",
    "development_events = 400\n",
    "\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train process\n",
    "## The train will modify one file and create three different files\n",
    "\n",
    "### Log File:\n",
    "This file will store basic information of all Package's trains and it will guide the analyses file to recognize which train information file should load. In each train this file should be appended with a new line contend the basic information to find the train information file (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Information File\n",
    "This file will store full information of the train performed (all parameters) in its name (each train information file will have a different name). And it will guide which train classifier file or which train result file should be open for analysis (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n",
    "\n",
    "### Train Result File\n",
    "This file will store the classifier result for all data and classification target (TXT FORMAT) or (PYTHON FORMAT) - This file should be access by all programs (MatLab and Python) for Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "Dividing data in train and test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/vinicius.mello/Workspace/SonarAnalysis/Results/Classification/NeuralNetwork/output_files/2017_08_14_19_41_47_train_info.jbl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"Classification\",'NeuralNetwork')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 4\n",
    "n_inits = 4\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "print 'Dividing data in train and test'\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, n_folds)\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "train_info['preprocessing_extraction_done'] = False\n",
    "train_info['preprocessing_analysis_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "train_info['dev'] = development_flag\n",
    "\n",
    "train_info_name = files_results_path+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Train\n",
      "Date: 2017_08_14_19_41_47\n",
      "Package: NeuralNetwork\n",
      "\n",
      "{'date': '2017_08_14_19_41_47', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_14_16_02_17', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_14_15_23_46', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_07_11_05_17', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_06_08_27_48', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_05_23_34_21', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_03_07_40_59', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_02_22_38_31', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_02_21_39_09', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_01_22_41_15', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_01_16_06_20', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_08_01_10_29_57', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_31_23_13_18', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_31_21_19_37', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_29_20_02_39', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_29_01_19_00', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_28_22_22_25', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_28_09_33_56', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_28_00_10_04', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_27_20_45_24', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_27_01_09_30', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_27_00_28_29', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_26_12_47_25', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_26_09_48_39', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_25_19_38_23', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_24_23_54_53', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_24_14_09_20', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_23_11_11_41', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_21_16_13_33', 'package': 'NeuralNetwork'}\n",
      "{'date': '2017_07_21_15_28_43', 'package': 'NeuralNetwork'}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"Classification\")\n",
    "lastTrain = log_entries[np.max(log_entries.keys())]\n",
    "print 'Last Train\\nDate: {0}\\nPackage: {1}\\n'.format(lastTrain['date'],lastTrain['package'])\n",
    "for ilog in log_entries.values()[::-1]:\n",
    "    if ilog['package'] == analysis_name:\n",
    "        print ilog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class TrnInformation\n",
      "\tDate 2017_08_14_19_41_47\n",
      "\tNumber of Folds 4\n",
      "\tNumber of Initializations: 4\n",
      "\tNormalization: mapstd\n",
      "\tCVO is not None\n",
      "\tVerbose is False\n",
      "\tTrain Verbose is False\n"
     ]
    }
   ],
   "source": [
    "a = class_anal.NeuralClassification(name         = analysis_name,\n",
    "                                    preproc_path = files_results_path,\n",
    "                                    train_path   = files_results_path,\n",
    "                                    anal_path    = files_results_path)\n",
    "date = lastTrain['date']\n",
    "n_folds = 4\n",
    "n_inits = 4\n",
    "a.trn_info = class_anal.TrnInformation(date     = date, \n",
    "                                       n_folds  = n_folds,\n",
    "                                       verbose  = False,\n",
    "                                       train_verbose= False,\n",
    "                                       n_inits  = n_inits,\n",
    "                                       patience = 25,\n",
    "                                       n_epochs = 500,\n",
    "                                       batch_size = 512)\n",
    "a.trn_info.SplitTrainSet(all_trgt)\n",
    "a.trn_info.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(parameters, fold): #data, trgt, n_neurons=1, trn_info=None, fold=0):\n",
    "        #print 'NeuralClassication train function'\n",
    "        data = parameters[0]\n",
    "        trgt = parameters[1]\n",
    "        n_neurons = parameters[2]\n",
    "        trn_info = parameters[3]\n",
    "\n",
    "        if fold > trn_info.n_folds or fold < -1:\n",
    "            print 'Invalid Fold...'\n",
    "            return None\n",
    "        \n",
    "        [data_preproc, trgt_preproc] = preprocess(data,trgt,\n",
    "                                                       trn_info=trn_info,fold=fold)\n",
    "        # Check if the file exists\n",
    "        file_name = '%s/%s_%s_train_fold_%i_neurons_%i_model.h5'%(preproc_path,\n",
    "                                                                  trn_info.date,\n",
    "                                                                  name,fold,n_neurons)\n",
    "        \n",
    "        if not os.path.exists(file_name):\n",
    "            best_init = 0\n",
    "            best_loss = 999\n",
    "            best_model = None\n",
    "            best_desc = {}\n",
    "            \n",
    "            train_id, test_id = trn_info.CVO[fold]\n",
    "            \n",
    "            for i_init in range(trn_info.n_inits):\n",
    "                print 'Init: %i of %i'%(i_init+1,trn_info.n_inits)\n",
    "                \n",
    "                model = Sequential()\n",
    "                model.add(Dense(n_neurons, input_dim=data.shape[1], init=\"uniform\"))\n",
    "                model.add(Activation('softplus'))\n",
    "                model.add(Dense(trgt_preproc.shape[1], init=\"uniform\"))\n",
    "                model.add(Activation('softmax'))\n",
    "        \n",
    "                adam = Adam(lr = trn_info.learning_rate,\n",
    "                            beta_2 = trn_info.beta_2,\n",
    "                epsilon = trn_info.epsilon)\n",
    "                \n",
    "                model.compile(loss='mean_squared_error',\n",
    "                    optimizer=adam,\n",
    "                    metrics=['accuracy'])\n",
    "            \n",
    "                # Train model\n",
    "                earlyStopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                        patience=trn_info.patience,\n",
    "                                                        verbose=trn_info.train_verbose,\n",
    "                                                        mode='auto')\n",
    "                init_trn_desc = model.fit(data_preproc[train_id], trgt_preproc[train_id],\n",
    "                                          nb_epoch=trn_info.n_epochs,\n",
    "                                          batch_size=trn_info.batch_size,\n",
    "                                          callbacks=[earlyStopping],\n",
    "                                          verbose=trn_info.verbose,\n",
    "                                          validation_data=(data_preproc[test_id],\n",
    "                                                           trgt_preproc[test_id]),\n",
    "                                          shuffle=True)\n",
    "        \t\n",
    "                if np.min(init_trn_desc.history['val_loss']) < best_loss:\n",
    "                    best_init = i_init\n",
    "                    best_loss = np.min(init_trn_desc.history['val_loss'])\n",
    "                    best_model = model\n",
    "                    best_desc['epochs'] = init_trn_desc.epoch\n",
    "                    best_desc['acc'] = init_trn_desc.history['acc']\n",
    "                    best_desc['loss'] = init_trn_desc.history['loss']\n",
    "                    best_desc['val_loss'] = init_trn_desc.history['val_loss']\n",
    "                    best_desc['val_acc'] = init_trn_desc.history['val_acc']\n",
    "        \n",
    "            # Save the model\n",
    "            file_name = '%s/%s_%s_train_fold_%i_neurons_%i_model.h5'%(train_path,\n",
    "                                                                      trn_info.date,\n",
    "                                                                      name,fold,n_neurons)\n",
    "            best_model.save(file_name)\n",
    "        \n",
    "            # Save the descriptor\n",
    "            file_name = '%s/%s_%s_train_fold_%i_neurons_%i_trn_desc.jbl'%(train_path,\n",
    "                                                                         trn_info.date,\n",
    "                                                                         name,fold,n_neurons)\n",
    "            joblib.dump([best_desc],file_name,compress=9)\n",
    "        else:\n",
    "            # Load the model\n",
    "            file_name = '%s/%s_%s_train_fold_%i_neurons_%i_model.h5'%(train_path,\n",
    "                                                                      trn_info.date,\n",
    "                                                                      name,fold,n_neurons)\n",
    "            best_model = load_model(file_name)\n",
    "        \n",
    "            # Load the descriptor\n",
    "            file_name = '%s/%s_%s_train_fold_%i_neurons_%i_trn_desc.jbl'%(train_path,\n",
    "                                                                     trn_info.date,\n",
    "                                                                     name,fold,n_neurons)\n",
    "            [best_desc] = joblib.load(file_name)\n",
    "        \n",
    "        return [best_model, best_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'n_folds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8700c08c772e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    248\u001b[0m         '''\n\u001b[1;32m    249\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'n_folds'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from Functions import ClassificationAnalysisMultiProcessing as class_anal\n",
    "\n",
    "\n",
    "n_neurons = 20\n",
    "\n",
    "parameters = np.zeros(4, dtype=object)\n",
    "# Constant parameters\n",
    "parameters[0] = all_data\n",
    "parameters[1] = trgt_sparse\n",
    "parameters[2] = n_neurons\n",
    "parameters[3] = a.trn_info\n",
    "\n",
    "def func(ifold, parameters, date = lastTrain['date'], a = class_anal.NeuralClassification(\n",
    "                                                                            name = analysis_name,\n",
    "                                                                            preproc_path = files_results_path,\n",
    "                                                                            train_path   = files_results_path,\n",
    "                                                                            anal_path    = files_results_path)):\n",
    "    n_folds = 4\n",
    "    n_inits = 4\n",
    "        \n",
    "    a.trn_info = class_anal.TrnInformation(date     = date, \n",
    "                                           n_folds  = n_folds,\n",
    "                                           verbose  = False,\n",
    "                                           train_verbose= False,\n",
    "                                           n_inits  = n_inits,\n",
    "                                           patience = 25,\n",
    "                                           n_epochs = 500,\n",
    "                                           batch_size = 512)\n",
    "    a.trn_info.SplitTrainSet(all_trgt)\n",
    "    #a.trn_info.Print()\n",
    "    return ifold, a.train(parameters = parameters, fold = ifold)\n",
    "    \n",
    "#### end func() ####\n",
    "\n",
    "num_processes = 4\n",
    "call = partial(func, parameters, date)\n",
    "p = Pool(processes = num_processes)\n",
    "n_folds = 4\n",
    "folds = range(n_folds)\n",
    "p.map(call, folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline  \n",
    "#a.analysis_top_sweep(all_data,trgt_sparse,trn_info=a.trn_info, min_neurons=10, max_neurons=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#[best_model, best_desc] = a.train(all_data,trgt_sparse,trn_infoexit=a.trn_info, n_neurons=25, fold=0)\n",
    "#loss_vector = best_desc['val_loss']\n",
    "#acc_vector  = best_desc['val_acc']\n",
    "#print loss_vector\n",
    "#print acc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# Analysis for independet topology with respect to number of neurons\n",
    "n_neurons = 30\n",
    "for ifold in range(n_folds):\n",
    "    a.analysis_train_plot(data = all_data,trgt = trgt_sparse,trn_info = a.trn_info, n_neurons= n_neurons, fold = ifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "print \"Results for %i neurons\"%n_neurons\n",
    "for ifold in range(n_folds):\n",
    "    a.analysis_conf_mat(data = all_data,\n",
    "                        trgt = trgt_sparse,\n",
    "                        trn_info = a.trn_info,\n",
    "                        class_labels = class_labels.values(),\n",
    "                        n_neurons = n_neurons,\n",
    "                        fold = ifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "print \"Results for %i neurons\"%n_neurons\n",
    "for ifold in range(n_folds):\n",
    "    a.analysis_output_hist(data = all_data,trgt = trgt_sparse,trn_info = a.trn_info, n_neurons= n_neurons, fold = ifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
